# ExecuTorch Android/AAOS ì ìš© ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì „ì²´ ì›Œí¬í”Œë¡œìš°](#ì „ì²´-ì›Œí¬í”Œë¡œìš°)
3. [Phase 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„](#phase-1-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì¤€ë¹„)
4. [Phase 2: ëª¨ë¸ ì¤€ë¹„](#phase-2-ëª¨ë¸-ì¤€ë¹„)
5. [Phase 3: Android ë°°í¬](#phase-3-android-ë°°í¬)
6. [Phase 4: Kotlin ì½”ë“œ êµ¬í˜„](#phase-4-kotlin-ì½”ë“œ-êµ¬í˜„)
7. [Backend ì „ëµ](#backend-ì „ëµ)
8. [Clean Architecture í†µí•©](#clean-architecture-í†µí•©)
9. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

## ê°œìš”

### ExecuTorchë€?
- PyTorch ëª¨ë¸ì„ ëª¨ë°”ì¼/ì„ë² ë””ë“œ ë””ë°”ì´ìŠ¤ì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ëŸ°íƒ€ì„
- `.pte` (PyTorch ExecuTorch) íŒŒì¼ í¬ë§· ì‚¬ìš©
- LLM(Llama, Gemma ë“±)ì„ ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥

### í•µì‹¬ êµ¬ì„±ìš”ì†Œ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kotlin/Java App (Android/AAOS)         â”‚
â”‚   â†“ JNI                                 â”‚
â”‚ ExecuTorch Runtime (C++)                â”‚
â”‚   â†“ Backend                             â”‚
â”‚ XNNPACK / Vulkan / Qualcomm QNN        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì£¼ìš” íŠ¹ì§•
- **Quantization**: INT4/INT8ë¡œ ëª¨ë¸ í¬ê¸° ì¶•ì†Œ (7B â†’ ~2GB)
- **KV Cache**: Auto-regressive ìƒì„± ìµœì í™”
- **Multi-Backend**: CPU(XNNPACK), GPU(Vulkan), NPU(Qualcomm) ì§€ì›

---

## ì „ì²´ ì›Œí¬í”Œë¡œìš°

```mermaid
graph TD
    A[Phase 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„] --> B[Phase 2: ëª¨ë¸ ì¤€ë¹„]
    B --> C[Phase 3: Android ë°°í¬]
    C --> D[Phase 4: Kotlin ì½”ë“œ êµ¬í˜„]
    
    A1[Gradle Dependency] --> A
    A2[NDK ë¹Œë“œ AAR] --> A
    
    B1[PyTorch ëª¨ë¸ ë‹¤ìš´ë¡œë“œ] --> B
    B2[PTE ë³€í™˜] --> B
    B3[Tokenizer ë³€í™˜] --> B
    
    C1[adb push ëª¨ë¸] --> C
    
    D1[Module.load] --> D
    D2[Auto-regressive Loop] --> D
```

### ì‹œê°„ ì†Œìš” ì˜ˆìƒ
- **Phase 1**: 2-4ì‹œê°„ (1íšŒë§Œ)
- **Phase 2**: 1-2ì‹œê°„ (ëª¨ë¸ë‹¹ 1íšŒ)
- **Phase 3**: 10ë¶„
- **Phase 4**: ê°œë°œ ê¸°ê°„ì— ë”°ë¼

---

## Phase 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„

### ëª©í‘œ
- ExecuTorch Runtimeì„ Androidì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„
- JNI Wrapper + Native Libraries (AAR íŒŒì¼)

### Step 1-1: ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

```bash
# í•„ìˆ˜ ì„¤ì¹˜
- Python 3.10-3.12
- Android NDK 26+
- CMake 3.19+
- Conda (ê¶Œì¥)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export ANDROID_NDK=$HOME/Library/Android/sdk/ndk/26.1.10909125
export ANDROID_HOME=$HOME/Library/Android/sdk
```

### Step 1-2: ExecuTorch í´ë¡ 

```bash
git clone https://github.com/pytorch/executorch.git
cd executorch
git checkout viable/strict  # ì•ˆì • ë²„ì „
```

### Step 1-3: NDK ë¹Œë“œ (Custom AAR ìƒì„±)

#### Option A: ìë™ ìŠ¤í¬ë¦½íŠ¸ (ê¶Œì¥)

```bash
cd examples/demo-apps/android/LlamaDemo
./setup.sh

# ê²°ê³¼: app/libs/executorch.aar ìƒì„±
```

#### Option B: ìˆ˜ë™ ë¹Œë“œ (ì»¤ìŠ¤í„°ë§ˆì´ì§• í•„ìš”ì‹œ)

```bash
# 1. CMake Configure
cmake -S . -B cmake-out-android \
    -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
    -DANDROID_ABI=arm64-v8a \
    -DANDROID_PLATFORM=android-23 \
    -DCMAKE_BUILD_TYPE=Release \
    -DEXECUTORCH_BUILD_XNNPACK=ON \
    -DEXECUTORCH_BUILD_KERNELS_OPTIMIZED=ON \
    -DEXECUTORCH_BUILD_EXTENSION_MODULE=ON

# 2. Build
cmake --build cmake-out-android -j$(nproc) --target install

# 3. AAR íŒ¨í‚¤ì§•
cd extension/android
./gradlew :executorch:assembleRelease

# 4. ê²°ê³¼ë¬¼ í™•ì¸
ls executorch/build/outputs/aar/executorch-release.aar
```

#### ìƒì„±ëœ AAR êµ¬ì¡°

```
executorch.aar
â”œâ”€â”€ jni/arm64-v8a/
â”‚   â”œâ”€â”€ libexecutorch.so          # Core runtime
â”‚   â”œâ”€â”€ libxnnpack_backend.so     # XNNPACK (CPU ìµœì í™”)
â”‚   â”œâ”€â”€ liboptimized_ops_lib.so   # Neon SIMD kernels
â”‚   â””â”€â”€ libextension_module.so    # Module API
â””â”€â”€ classes.jar                    # Java JNI wrapper
```

### Step 1-4: í”„ë¡œì íŠ¸ì— AAR ì¶”ê°€

```kotlin
// app/build.gradle.kts
android {
    defaultConfig {
        ndk {
            abiFilters += "arm64-v8a"
        }
    }
}

dependencies {
    // Custom AAR ì‚¬ìš©
    implementation(files("libs/executorch.aar"))
}
```

### Step 1-5: AAR ê²€ì¦

```bash
# AAR ì••ì¶• í•´ì œ
unzip executorch.aar -d executorch-aar

# Native ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
ls executorch-aar/jni/arm64-v8a/
# libexecutorch.so, libxnnpack_backend.so ë“±ì´ ìˆì–´ì•¼ í•¨
```

---

## Phase 2: ëª¨ë¸ ì¤€ë¹„

### ëª©í‘œ
- PyTorch ëª¨ë¸ â†’ `.pte` íŒŒì¼ ë³€í™˜
- Tokenizer â†’ `.bin` íŒŒì¼ ë³€í™˜

### Step 2-1: Python í™˜ê²½ ì„¤ì •

```bash
# Conda í™˜ê²½ ìƒì„±
conda create -n executorch python=3.10
conda activate executorch

# ExecuTorch ì„¤ì¹˜
cd executorch
pip install -e .
./install_requirements.sh

# LLM export ì˜ì¡´ì„±
pip install torch torchvision transformers
```

### Step 2-2: Llama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# HuggingFace CLI ì„¤ì¹˜
pip install huggingface-hub

# Llama 3.2 1B ë‹¤ìš´ë¡œë“œ (ì˜ˆì‹œ)
huggingface-cli download \
    meta-llama/Llama-3.2-1B-Instruct \
    --local-dir ./llama-3.2-1b \
    --include "*.safetensors" "tokenizer.model" "*.json"

# ë‹¤ìš´ë¡œë“œ ê²°ê³¼:
# ./llama-3.2-1b/
# â”œâ”€â”€ model.safetensors
# â”œâ”€â”€ tokenizer.model
# â”œâ”€â”€ tokenizer_config.json
# â””â”€â”€ config.json
```

### Step 2-3: PTE ë³€í™˜ (Export + Quantization)

#### YAML ì„¤ì • íŒŒì¼ ì‘ì„±

```yaml
# config/llama3_2_1b_xnnpack.yaml
base:
  model_class: "llama3_2_1b"
  checkpoint: "~/llama-3.2-1b/model.safetensors"
  params: "~/llama-3.2-1b/config.json"

model:
  use_kv_cache: true
  use_sdpa_with_kv_cache: true
  max_seq_length: 2048

backend:
  xnnpack:
    enabled: true
    extended_ops: true

quantization:
  qmode: "8da4w"  # 8-bit dynamic activation + 4-bit weight
  group_size: 128

export:
  output_name: "llama3_2_1b.pte"
```

#### Export ì‹¤í–‰

```bash
python -m extension.llm.export.export_llm \
    --config config/llama3_2_1b_xnnpack.yaml

# ê²°ê³¼: llama3_2_1b.pte (~500MB, INT4 quantized)
```

#### ëª…ë ¹ì¤„ ë°©ì‹ (YAML ì—†ì´)

```bash
python -m extension.llm.export.export_llm \
    --checkpoint ~/llama-3.2-1b/model.safetensors \
    --params ~/llama-3.2-1b/config.json \
    --output_name llama3_2_1b.pte \
    --use_kv_cache \
    --use_sdpa_with_kv_cache \
    --xnnpack \
    --quantization 8da4w \
    --group_size 128 \
    --max_seq_length 2048
```

### Step 2-4: Tokenizer ë³€í™˜ (ì„ íƒì‚¬í•­)

```bash
# Llama 3.2ëŠ” tokenizer.model ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥
# ë³€í™˜ì´ í•„ìš”í•œ ê²½ìš°:
python -m pytorch_tokenizers.tools.llama2c.convert \
    -t ~/llama-3.2-1b/tokenizer.model \
    -o tokenizer.bin
```

### Step 2-5: Python Runtime ê²€ì¦ (ì„ íƒì‚¬í•­)

```python
from executorch.runtime import Runtime

# ëª¨ë¸ ë¡œë“œ
runtime = Runtime.get()
program = runtime.load_program("llama3_2_1b.pte")
method = program.load_method("forward")

# í…ŒìŠ¤íŠ¸ ì¶”ë¡ 
import torch
input_tensor = torch.randint(0, 128256, (1, 10), dtype=torch.long)
output = method.execute([input_tensor])
print("âœ… PTE ê²€ì¦ ì„±ê³µ!")
```

### Step 2-6: ë¹ ë¥¸ ë°©ë²• - Pre-exported PTE ë‹¤ìš´ë¡œë“œ

```bash
# HuggingFaceì—ì„œ ì´ë¯¸ exportëœ PTE ë‹¤ìš´ë¡œë“œ
huggingface-cli download \
    executorch-community/Llama-3.2-1B-Instruct-4bit-xnnpack \
    --local-dir ./models

# ê²°ê³¼:
# ./models/
# â”œâ”€â”€ llama3_2_1b.pte  (ì´ë¯¸ export & quantized!)
# â””â”€â”€ tokenizer.bin

# Step 2-3, 2-4ë¥¼ ê±´ë„ˆë›¸ ìˆ˜ ìˆìŒ
```

---

## Phase 3: Android ë°°í¬

### ëª©í‘œ
- PTE íŒŒì¼ê³¼ Tokenizerë¥¼ Android ë””ë°”ì´ìŠ¤ë¡œ ì „ì†¡

### Step 3-1: ë””ë°”ì´ìŠ¤ ì—°ê²° í™•ì¸

```bash
adb devices
# List of devices attached
# 12345678    device
```

### Step 3-2: ë””ë ‰í† ë¦¬ ìƒì„± ë° íŒŒì¼ ì—…ë¡œë“œ

```bash
# ë””ë ‰í† ë¦¬ ìƒì„±
adb shell mkdir -p /data/local/tmp/llama

# ëª¨ë¸ & í† í¬ë‚˜ì´ì € ì—…ë¡œë“œ
adb push llama3_2_1b.pte /data/local/tmp/llama/
adb push tokenizer.bin /data/local/tmp/llama/

# ì—…ë¡œë“œ í™•ì¸
adb shell ls -lh /data/local/tmp/llama/
# -rw-rw-rw- 1 shell shell 487M llama3_2_1b.pte
# -rw-rw-rw- 1 shell shell 500K tokenizer.bin
```

### Step 3-3: ê¶Œí•œ ì„¤ì • (í•„ìš”ì‹œ)

```bash
adb shell chmod 644 /data/local/tmp/llama/*
```

### Step 3-4: Assets ë²ˆë“¤ë§ (ëŒ€ì•ˆ)

ì•± ë¹Œë“œ ì‹œ í¬í•¨í•˜ë ¤ë©´:

```kotlin
// app/src/main/assets/
assets/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ llama3_2_1b.pte
â””â”€â”€ tokenizers/
    â””â”€â”€ tokenizer.bin
```

```kotlin
// ëŸ°íƒ€ì„ì— ë‚´ë¶€ ì €ì¥ì†Œë¡œ ë³µì‚¬
private fun copyAssetsToInternalStorage() {
    val modelsDir = File(context.filesDir, "models")
    if (!modelsDir.exists()) {
        modelsDir.mkdirs()
        context.assets.open("models/llama3_2_1b.pte").use { input ->
            FileOutputStream(File(modelsDir, "llama3_2_1b.pte")).use { output ->
                input.copyTo(output)
            }
        }
    }
}
```

---

## Phase 4: Kotlin ì½”ë“œ êµ¬í˜„

### ëª©í‘œ
- ExecuTorch Module APIë¡œ LLM ì¶”ë¡  êµ¬í˜„
- Auto-regressive ìƒì„± ë£¨í”„ ì‘ì„±

### Step 4-1: ê¸°ë³¸ ì¶”ë¡  ì½”ë“œ

```kotlin
// data/datasource/LlamaDataSource.kt
package com.example.aaos.data.datasource

import org.pytorch.executorch.Module
import org.pytorch.executorch.EValue
import org.pytorch.executorch.Tensor
import android.util.Log

class LlamaDataSource(
    private val ptePath: String,
    private val tokenizerPath: String
) {
    private val TAG = "LlamaDataSource"
    
    private val module: Module by lazy {
        Log.d(TAG, "Loading model from: $ptePath")
        Module.load(ptePath)
    }
    
    private val tokenizer: Tokenizer by lazy {
        Tokenizer.load(tokenizerPath)
    }
    
    /**
     * í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì‘ë‹µ ìƒì„±
     */
    fun generate(prompt: String, maxTokens: Int = 100): String {
        // 1. í”„ë¡¬í”„íŠ¸ë¥¼ í† í° IDë¡œ ë³€í™˜
        val inputTokens = tokenizer.encode(prompt).toMutableList()
        Log.d(TAG, "Input tokens: ${inputTokens.size}")
        
        // 2. Auto-regressive ìƒì„±
        repeat(maxTokens) { step ->
            // 2-1. í† í° ë°°ì—´ â†’ Long í…ì„œ
            val inputArray = inputTokens.toLongArray()
            val inputTensor = Tensor.fromBlob(
                inputArray,
                longArrayOf(1, inputArray.size.toLong())
            )
            
            // 2-2. ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
            val outputs = module.forward(EValue.from(inputTensor))
            val logits = outputs[0].toTensor().dataAsFloatArray
            
            // 2-3. ë‹¤ìŒ í† í° ìƒ˜í”Œë§
            val vocabSize = 128256  // Llama 3.2
            val lastLogits = logits.takeLast(vocabSize)
            val nextToken = sampleToken(lastLogits)
            
            // 2-4. EOS í† í°ì´ë©´ ì¢…ë£Œ
            if (nextToken == tokenizer.eosTokenId) {
                Log.d(TAG, "EOS reached at step $step")
                break
            }
            
            inputTokens.add(nextToken)
            
            if (step % 10 == 0) {
                Log.d(TAG, "Generated $step tokens")
            }
        }
        
        // 3. í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
        return tokenizer.decode(inputTokens)
    }
    
    /**
     * ìƒ˜í”Œë§ ì „ëµ (Greedy)
     */
    private fun sampleToken(logits: List<Float>): Int {
        // Greedy sampling - ê°€ì¥ í™•ë¥  ë†’ì€ í† í° ì„ íƒ
        return logits.indices.maxByOrNull { logits[it] } ?: 0
    }
    
    /**
     * Temperature Sampling (ì„ íƒì‚¬í•­)
     */
    private fun sampleTokenWithTemperature(
        logits: List<Float>, 
        temperature: Float = 0.8f
    ): Int {
        val scaledLogits = logits.map { it / temperature }
        val expLogits = scaledLogits.map { kotlin.math.exp(it.toDouble()).toFloat() }
        val sumExp = expLogits.sum()
        val probs = expLogits.map { it / sumExp }
        
        // Categorical sampling
        val random = kotlin.random.Random.nextFloat()
        var cumSum = 0f
        for (i in probs.indices) {
            cumSum += probs[i]
            if (random < cumSum) return i
        }
        return probs.size - 1
    }
}
```

### Step 4-2: Tokenizer êµ¬í˜„

```kotlin
// data/tokenizer/Tokenizer.kt
package com.example.aaos.data.tokenizer

import java.io.File

interface Tokenizer {
    fun encode(text: String): List<Int>
    fun decode(tokens: List<Int>): String
    val eosTokenId: Int
    val bosTokenId: Int
    
    companion object {
        fun load(path: String): Tokenizer {
            // SentencePiece ë˜ëŠ” Tiktoken êµ¬í˜„
            return SentencePieceTokenizer(path)
        }
    }
}

class SentencePieceTokenizer(private val modelPath: String) : Tokenizer {
    // Native JNI í˜¸ì¶œ ë˜ëŠ” pure Kotlin êµ¬í˜„
    // ì°¸ê³ : https://github.com/google/sentencepiece
    
    override val eosTokenId: Int = 128001
    override val bosTokenId: Int = 128000
    
    override fun encode(text: String): List<Int> {
        // TODO: SentencePiece êµ¬í˜„
        return emptyList()
    }
    
    override fun decode(tokens: List<Int>): String {
        // TODO: SentencePiece êµ¬í˜„
        return ""
    }
}
```

### Step 4-3: Repository Layer (Clean Architecture)

```kotlin
// domain/repository/LLMRepository.kt
package com.example.aaos.domain.repository

import kotlinx.coroutines.flow.Flow

interface LLMRepository {
    suspend fun generateResponse(prompt: String): Flow<String>
    fun getSupportedBackends(): List<String>
}
```

```kotlin
// data/repository/LLMRepositoryImpl.kt
package com.example.aaos.data.repository

import com.example.aaos.data.datasource.LlamaDataSource
import com.example.aaos.domain.repository.LLMRepository
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.flow
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext

class LLMRepositoryImpl(
    private val llamaDataSource: LlamaDataSource
) : LLMRepository {
    
    override suspend fun generateResponse(prompt: String): Flow<String> = flow {
        val response = withContext(Dispatchers.Default) {
            llamaDataSource.generate(prompt, maxTokens = 100)
        }
        emit(response)
    }
    
    override fun getSupportedBackends(): List<String> {
        return listOf("XNNPACK", "Vulkan", "Qualcomm QNN")
    }
}
```

### Step 4-4: ViewModel (Compose)

```kotlin
// presentation/viewmodel/ChatViewModel.kt
package com.example.aaos.presentation.viewmodel

import androidx.lifecycle.ViewModel
import androidx.lifecycle.viewModelScope
import com.example.aaos.domain.repository.LLMRepository
import kotlinx.coroutines.flow.MutableStateFlow
import kotlinx.coroutines.flow.asStateFlow
import kotlinx.coroutines.launch

class ChatViewModel(
    private val repository: LLMRepository
) : ViewModel() {
    
    private val _response = MutableStateFlow("")
    val response = _response.asStateFlow()
    
    private val _isLoading = MutableStateFlow(false)
    val isLoading = _isLoading.asStateFlow()
    
    fun sendPrompt(prompt: String) {
        viewModelScope.launch {
            _isLoading.value = true
            _response.value = ""
            
            try {
                repository.generateResponse(prompt).collect { text ->
                    _response.value = text
                }
            } catch (e: Exception) {
                _response.value = "Error: ${e.message}"
            } finally {
                _isLoading.value = false
            }
        }
    }
}
```

### Step 4-5: Compose UI

```kotlin
// presentation/ui/ChatScreen.kt
package com.example.aaos.presentation.ui

import androidx.compose.foundation.layout.*
import androidx.compose.material3.*
import androidx.compose.runtime.*
import androidx.compose.ui.Modifier
import androidx.compose.ui.unit.dp

@Composable
fun ChatScreen(viewModel: ChatViewModel) {
    val response by viewModel.response.collectAsState()
    val isLoading by viewModel.isLoading.collectAsState()
    
    var userInput by remember { mutableStateOf("") }
    
    Column(
        modifier = Modifier
            .fillMaxSize()
            .padding(16.dp)
    ) {
        // Response display
        Card(
            modifier = Modifier
                .weight(1f)
                .fillMaxWidth()
        ) {
            Text(
                text = response.ifEmpty { "Response will appear here..." },
                modifier = Modifier.padding(16.dp)
            )
        }
        
        Spacer(modifier = Modifier.height(16.dp))
        
        // Input field
        OutlinedTextField(
            value = userInput,
            onValueChange = { userInput = it },
            modifier = Modifier.fillMaxWidth(),
            label = { Text("Enter your prompt") },
            enabled = !isLoading
        )
        
        Spacer(modifier = Modifier.height(8.dp))
        
        // Send button
        Button(
            onClick = {
                viewModel.sendPrompt(userInput)
                userInput = ""
            },
            modifier = Modifier.fillMaxWidth(),
            enabled = !isLoading && userInput.isNotBlank()
        ) {
            if (isLoading) {
                CircularProgressIndicator(
                    modifier = Modifier.size(24.dp),
                    color = MaterialTheme.colorScheme.onPrimary
                )
            } else {
                Text("Send")
            }
        }
    }
}
```

### Step 4-6: Dependency Injection (Hilt)

```kotlin
// di/AppModule.kt
@Module
@InstallIn(SingletonComponent::class)
object AppModule {
    
    @Provides
    @Singleton
    fun provideLlamaDataSource(
        @ApplicationContext context: Context
    ): LlamaDataSource {
        val ptePath = "/data/local/tmp/llama/llama3_2_1b.pte"
        val tokenizerPath = "/data/local/tmp/llama/tokenizer.bin"
        return LlamaDataSource(ptePath, tokenizerPath)
    }
    
    @Provides
    @Singleton
    fun provideLLMRepository(
        dataSource: LlamaDataSource
    ): LLMRepository {
        return LLMRepositoryImpl(dataSource)
    }
}
```

---

## Backend ì „ëµ

### Backendë€?
ExecuTorchê°€ ëª¨ë¸ì„ ì‹¤í–‰í•  í•˜ë“œì›¨ì–´ë¥¼ ì„ íƒí•˜ëŠ” ë°©ì‹

### ì§€ì› Backend

| Backend | íƒ€ê²Ÿ | íŠ¹ì§• | ì„±ëŠ¥ |
|---------|------|------|------|
| **XNNPACK** | CPU (Arm, x86) | ë²”ìš©, ì•ˆì •ì  | 5-10 tokens/sec |
| **Vulkan** | GPU | GPU ê°€ì† | 8-15 tokens/sec |
| **Qualcomm QNN** | Snapdragon NPU | ìµœê³  ì„±ëŠ¥ | 15-25 tokens/sec |
| **MediaTek** | Dimensity APU | MediaTek ì „ìš© | 12-20 tokens/sec |
| **CoreML** | iOS Neural Engine | Apple ì „ìš© | 20-30 tokens/sec |

### Backend ì„¤ì • 3ë‹¨ê³„

#### 1ï¸âƒ£ PTE Export ì‹œ Backend ì§€ì •

```yaml
# config/llama_xnnpack.yaml
backend:
  xnnpack:
    enabled: true
    extended_ops: true
```

```yaml
# config/llama_vulkan.yaml
backend:
  vulkan:
    enabled: true
```

```yaml
# config/llama_qnn.yaml
backend:
  qnn:
    enabled: true
    soc_model: "SM8550"  # Snapdragon 8 Gen 2
```

#### 2ï¸âƒ£ NDK ë¹Œë“œ ì‹œ Backend ì»´íŒŒì¼

```bash
# XNNPACKë§Œ
cmake -DEXECUTORCH_BUILD_XNNPACK=ON

# Vulkan ì¶”ê°€
cmake -DEXECUTORCH_BUILD_XNNPACK=ON \
      -DEXECUTORCH_BUILD_VULKAN=ON

# Qualcomm QNN ì¶”ê°€
cmake -DEXECUTORCH_BUILD_QNN=ON \
      -DQNN_SDK_ROOT=/path/to/qnn/sdk
```

#### 3ï¸âƒ£ Runtime ë¡œë“œ (ìë™)

```kotlin
// PTE íŒŒì¼ì— backend ì •ë³´ê°€ ì„ë² ë“œë˜ì–´ ìˆì–´ì„œ ìë™ ì„ íƒ
val module = Module.load("/path/to/llama_xnnpack.pte")  // XNNPACK ì‚¬ìš©
val module = Module.load("/path/to/llama_qnn.pte")      // QNN ì‚¬ìš©
```

### ê¶Œì¥ ì „ëµ: Multi-Backend ì§€ì›

#### ë””ë°”ì´ìŠ¤ë³„ Backend ê°ì§€

```kotlin
// data/backend/BackendDetector.kt
object BackendDetector {
    
    enum class Backend(val pteFileName: String) {
        QUALCOMM("llama_qnn.pte"),
        VULKAN("llama_vulkan.pte"),
        XNNPACK("llama_xnnpack.pte")
    }
    
    fun detectBestBackend(): Backend {
        val soc = getSocModel()
        
        return when {
            isQualcommDevice(soc) -> Backend.QUALCOMM
            supportsVulkan() -> Backend.VULKAN
            else -> Backend.XNNPACK
        }
    }
    
    private fun getSocModel(): String {
        return try {
            val process = Runtime.getRuntime()
                .exec("getprop ro.board.platform")
            process.inputStream.bufferedReader().readText().trim()
        } catch (e: Exception) {
            "unknown"
        }
    }
    
    private fun isQualcommDevice(soc: String): Boolean {
        return soc.lowercase().startsWith("sm8") ||  // Snapdragon 8
               soc.lowercase().startsWith("kalama")   // Chipset codename
    }
    
    private fun supportsVulkan(): Boolean {
        return File("/system/lib64/libvulkan.so").exists()
    }
}
```

#### Fallback Chain êµ¬í˜„

```kotlin
fun selectBackendWithFallback(modelsDir: String): Backend {
    val priority = listOf(
        Backend.QUALCOMM,  // ìµœê³  ìš°ì„ ìˆœìœ„
        Backend.VULKAN,
        Backend.XNNPACK    // Fallback
    )
    
    for (backend in priority) {
        val ptePath = "$modelsDir/${backend.pteFileName}"
        if (File(ptePath).exists()) {
            Log.i(TAG, "Selected: ${backend.name}")
            return backend
        }
    }
    
    return Backend.XNNPACK  // ìµœí›„ì˜ fallback
}
```

### AAOS ê¶Œì¥ êµ¬ì„±

```bash
# 1. XNNPACK (í•„ìˆ˜ - ëª¨ë“  ë””ë°”ì´ìŠ¤)
python -m extension.llm.export.export_llm \
    --config config/llama_xnnpack.yaml

# 2. Qualcomm (ì„ íƒ - Snapdragon ë””ë°”ì´ìŠ¤)
python -m extension.llm.export.export_llm \
    --config config/llama_qnn.yaml

# ê²°ê³¼:
# llama_xnnpack.pte  (~500MB) - Fallback
# llama_qnn.pte      (~450MB) - Snapdragon ìµœì í™”
```

---

## Clean Architecture í†µí•©

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```
app/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ datasource/
â”‚   â”‚   â””â”€â”€ LlamaDataSource.kt          # ExecuTorch Module ë˜í¼
â”‚   â”œâ”€â”€ repository/
â”‚   â”‚   â””â”€â”€ LLMRepositoryImpl.kt
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â””â”€â”€ BackendDetector.kt
â”‚   â””â”€â”€ tokenizer/
â”‚       â””â”€â”€ Tokenizer.kt
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ repository/
â”‚   â”‚   â””â”€â”€ LLMRepository.kt
â”‚   â””â”€â”€ usecase/
â”‚       â””â”€â”€ GenerateResponseUseCase.kt
â””â”€â”€ presentation/
    â”œâ”€â”€ viewmodel/
    â”‚   â””â”€â”€ ChatViewModel.kt
    â””â”€â”€ ui/
        â””â”€â”€ ChatScreen.kt
```

### Layerë³„ ì±…ì„

#### Data Layer
```kotlin
// ExecuTorchì™€ ì§ì ‘ ìƒí˜¸ì‘ìš©
class LlamaDataSource(ptePath: String, tokenizerPath: String) {
    private val module = Module.load(ptePath)
    fun generate(prompt: String): String { /* ... */ }
}
```

#### Domain Layer
```kotlin
// ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§, í”Œë«í¼ ë…ë¦½ì 
interface LLMRepository {
    suspend fun generateResponse(prompt: String): Flow<String>
}

class GenerateResponseUseCase(private val repository: LLMRepository) {
    suspend operator fun invoke(prompt: String) = 
        repository.generateResponse(prompt)
}
```

#### Presentation Layer
```kotlin
// UI ìƒíƒœ ê´€ë¦¬
class ChatViewModel(private val useCase: GenerateResponseUseCase) {
    val response = MutableStateFlow("")
    fun sendPrompt(prompt: String) { /* ... */ }
}
```

---

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜

#### 1. `Module.load()` ì‹¤íŒ¨

```
Error: Failed to load model from /path/to/model.pte
```

**í•´ê²°ì±…:**
```kotlin
// íŒŒì¼ ì¡´ì¬ í™•ì¸
val pteFile = File(ptePath)
if (!pteFile.exists()) {
    Log.e(TAG, "PTE file not found: $ptePath")
    // assetsì—ì„œ ë³µì‚¬ ë˜ëŠ” ë‹¤ìš´ë¡œë“œ
}

// ê¶Œí•œ í™•ì¸
if (!pteFile.canRead()) {
    Log.e(TAG, "No read permission for: $ptePath")
}
```

#### 2. Backend ë¯¸ì§€ì›

```
Error: XnnpackBackend not found
```

**í•´ê²°ì±…:**
- NDK ë¹Œë“œ ì‹œ í•´ë‹¹ backendë¥¼ ì»´íŒŒì¼í–ˆëŠ”ì§€ í™•ì¸
- AAR ë‚´ë¶€ì— `libxnnpack_backend.so` ì¡´ì¬ í™•ì¸

```bash
unzip -l executorch.aar | grep xnnpack
# jni/arm64-v8a/libxnnpack_backend.so ìˆì–´ì•¼ í•¨
```

#### 3. Out of Memory

```
Error: Failed to allocate tensor memory
```

**í•´ê²°ì±…:**
- ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© (1B â†’ 500MB)
- Quantization ê°•í™” (INT8 â†’ INT4)
- `max_seq_length` ì¶•ì†Œ

```yaml
model:
  max_seq_length: 1024  # 2048 â†’ 1024ë¡œ ì¶•ì†Œ
```

#### 4. ëŠë¦° ì¶”ë¡  ì†ë„

**ìµœì í™” ë°©ë²•:**
1. KV Cache í™œì„±í™” í™•ì¸
2. SDPA í™œì„±í™” í™•ì¸
3. Backend ìµœì í™” (QNN > Vulkan > XNNPACK)
4. Batch size ì¡°ì •

```yaml
model:
  use_kv_cache: true              # í•„ìˆ˜
  use_sdpa_with_kv_cache: true    # í•„ìˆ˜
```

### ë””ë²„ê¹… íŒ

#### Logcat í•„í„°ë§

```bash
# ExecuTorch ë¡œê·¸ë§Œ ë³´ê¸°
adb logcat | grep -i executorch

# ì—ëŸ¬ë§Œ ë³´ê¸°
adb logcat *:E | grep -i executorch
```

#### PTE íŒŒì¼ ê²€ì¦

```python
from executorch.devtools.etrecord import parse_etrecord

# PTE ë©”íƒ€ë°ì´í„° í™•ì¸
record = parse_etrecord("llama3_2_1b.pte")
print(f"Backend: {record.backend}")
print(f"Operators: {record.operators}")
print(f"Memory: {record.memory_usage}")
```

#### ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§

```kotlin
class PerformanceMonitor {
    fun measureInference(block: () -> Unit): Long {
        val start = System.currentTimeMillis()
        block()
        val end = System.currentTimeMillis()
        return end - start
    }
}

// ì‚¬ìš©
val latency = monitor.measureInference {
    module.forward(input)
}
Log.d(TAG, "Inference latency: ${latency}ms")
```

---

## ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê°œë°œ í™˜ê²½ ì„¤ì • (1íšŒ)
- [ ] Android NDK ì„¤ì¹˜
- [ ] Python 3.10+ & Conda í™˜ê²½
- [ ] ExecuTorch í´ë¡ 
- [ ] Custom AAR ë¹Œë“œ ì„±ê³µ

### ëª¨ë¸ ì¤€ë¹„ (ëª¨ë¸ë‹¹ 1íšŒ)
- [ ] PyTorch ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (HuggingFace)
- [ ] PTEë¡œ export (quantization)
- [ ] Tokenizer ë³€í™˜ ë˜ëŠ” í™•ë³´
- [ ] Python runtime ê²€ì¦

### Android í†µí•©
- [ ] AARì„ í”„ë¡œì íŠ¸ì— ì¶”ê°€
- [ ] PTE íŒŒì¼ ë””ë°”ì´ìŠ¤ì— ì—…ë¡œë“œ
- [ ] Tokenizer íŒŒì¼ ë””ë°”ì´ìŠ¤ì— ì—…ë¡œë“œ
- [ ] íŒŒì¼ ê¶Œí•œ í™•ì¸

### ì½”ë“œ êµ¬í˜„
- [ ] LlamaDataSource êµ¬í˜„
- [ ] Tokenizer ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
- [ ] Repository & UseCase êµ¬í˜„
- [ ] ViewModel & Compose UI êµ¬í˜„
- [ ] Dependency Injection ì„¤ì •

### ìµœì í™” (ì„ íƒ)
- [ ] Multi-backend ì§€ì›
- [ ] Backend ìë™ ê°ì§€
- [ ] KV Cache í™œì„±í™”
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

---

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [ExecuTorch ê³µì‹ ë¬¸ì„œ](https://pytorch.org/executorch/stable/)
- [Getting Started Guide](https://pytorch.org/executorch/main/getting-started.html)
- [Llama Example](https://github.com/pytorch/executorch/tree/main/examples/models/llama)
- [Android í†µí•© ê°€ì´ë“œ](https://pytorch.org/executorch/main/using-executorch-android.html)

### HuggingFace ë¦¬ì†ŒìŠ¤
- [Pre-exported PTE ëª¨ë¸](https://huggingface.co/executorch-community)
- [Optimum-ExecuTorch](https://github.com/huggingface/optimum-executorch)

### ì˜ˆì œ ì½”ë“œ
- [Android Llama Demo](https://github.com/pytorch/executorch/tree/main/examples/demo-apps/android/LlamaDemo)
- [ExecuTorch Examples](https://github.com/pytorch/executorch/tree/main/examples)

---

## ë‹¤ìŒ ë‹¨ê³„

### Phase 1 ì™„ë£Œ í›„
âœ… AAR ë¹Œë“œ ì„±ê³µ
â†’ Phase 2ë¡œ ì§„í–‰ (ëª¨ë¸ ì¤€ë¹„)

### Phase 2 ì™„ë£Œ í›„
âœ… PTE íŒŒì¼ ìƒì„±
â†’ Phase 3ìœ¼ë¡œ ì§„í–‰ (ë””ë°”ì´ìŠ¤ ë°°í¬)

### Phase 3 ì™„ë£Œ í›„
âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ
â†’ Phase 4ë¡œ ì§„í–‰ (ì½”ë“œ êµ¬í˜„)

### Phase 4 ì™„ë£Œ í›„
âœ… ê¸°ë³¸ ì¶”ë¡  ë™ì‘
â†’ ìµœì í™” ë‹¨ê³„:
- Multi-backend ì§€ì›
- Streaming ì‘ë‹µ
- ë©”ëª¨ë¦¬ ìµœì í™”
- UI/UX ê°œì„ 

---

## ì‹œê°„ ì ˆì•½ íŒ

### ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…
1. Pre-exported PTE ì‚¬ìš© (Phase 2 ìŠ¤í‚µ)
2. Maven AAR ì‚¬ìš© (Phase 1 ê°„ì†Œí™”)
3. XNNPACKë§Œ ì‚¬ìš© (Backend ë‹¨ìˆœí™”)

```bash
# 1ì‹œê°„ ì•ˆì— í”„ë¡œí† íƒ€ì…
# 1. Maven AAR (build.gradle.kts)
implementation("org.pytorch:executorch-android:0.4.0")

# 2. Pre-exported PTE ë‹¤ìš´ë¡œë“œ
huggingface-cli download executorch-community/Llama-3.2-1B

# 3. adb push
adb push llama3_2_1b.pte /data/local/tmp/

# 4. Kotlin ì½”ë“œ ì‘ì„±
Module.load("/data/local/tmp/llama3_2_1b.pte")
```

### í”„ë¡œë•ì…˜ ì¤€ë¹„
1. Custom NDK ë¹Œë“œ (ìµœì í™”)
2. Multi-backend ì§€ì›
3. Backend ìë™ ê°ì§€
4. ì—ëŸ¬ í•¸ë“¤ë§ & ë¡œê¹…

---

## ì¶”ê°€ ê³ ë ¤ì‚¬í•­

### AAOS íŠ¹í™”
- ì°¨ëŸ‰ í™˜ê²½ì—ì„œì˜ ë©”ëª¨ë¦¬ ì œì•½
- ì˜¨ë„/ì§„ë™ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”
- ì•ˆì „ ìš´ì „ì„ ìœ„í•œ ì‘ë‹µ ì‹œê°„ ìµœì í™”
- ì˜¤í”„ë¼ì¸ ë™ì‘ ë³´ì¥

### ë³´ì•ˆ
- PTE íŒŒì¼ ì•”í˜¸í™” (AES)
- ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ë¡œê¹… ì •ì±…
- ë””ë°”ì´ìŠ¤ ë‚´ ë°ì´í„° ë³´í˜¸

### ì‚¬ìš©ì ê²½í—˜
- ì²« ì‘ë‹µ ì‹œê°„ (TTFT) ìµœì†Œí™”
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (í† í° ë‹¨ìœ„)
- ë°±ê·¸ë¼ìš´ë“œ ì´ˆê¸°í™”
- ì˜¤ë¥˜ ì‹œ ì¹œì ˆí•œ ë©”ì‹œì§€

---

**ì‘ì„±ì¼**: 2025-11-05  
**ExecuTorch ë²„ì „**: 0.4.0  
**ëŒ€ìƒ í”Œë«í¼**: Android/AAOS (Arm64)

ëª¨ë¸ ì¶”ì²œ)
https://huggingface.co/Motif-Technologies/Motif-2.6b-v1.1-LC
